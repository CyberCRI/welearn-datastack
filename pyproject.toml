[project]
name = "welearn-datastack"
version = "1.3.0"
description = "Data stack for WeLearn LPI projects. This pipeline can collect, vectorize and store data from various sources."
authors = [
    { name = "ThÃ©o Nardin", email = "theo.nardin@learningplanetinstitute.org" },
    { name = "Jean-Marc Sevin", email = "jean-marc.sevin@learningplanetinstitute.org" },
    { name = "Sandra Guerreiro", email = "sandra.guerreiro@learningplanetinstitute.org " },
]
maintainers = [
    { name = "WeLearn team", email = "welearn@learningplanetinstitute.org" },
]
requires-python = ">=3.12"
#license = { file = "LICENSE" } https://github.com/dependabot/dependabot-core/issues/12052
readme = "README.md"
dynamic = [
    "dependencies"
]

[tool.poetry]
package-mode = false


[tool.poetry.dependencies]
python = ">=3.12,<3.13"
sqlalchemy = "^2.0.46"
numpy = "^2.4.2"
requests = "^2.32.5"
wikipedia-api = "^0.9.0"
sentence-transformers = "^5.2.2"
spacy = "^3.8.11"
refinedoc = "^1.0.0"
qdrant-client = "1.16.2"
python-dotenv = "^1.1.0"
beautifulsoup4 = "^4.14.3"
pyphen = "^0.17.2"
ijson = "^3.4.0"
keybert = "^0.9.0"
torch = {version="^2.10.0+cpu", source = "pytorch_cpu"}
torchvision = {version="^0.25.0+cpu", source = "pytorch_cpu"}
xx_sent_ud_sm = {url = "https://github.com/explosion/spacy-models/releases/download/xx_sent_ud_sm-3.8.0/xx_sent_ud_sm-3.8.0-py3-none-any.whl"}

lingua-language-detector = "^2.1.1"
psycopg2-binary = "^2.9.10"
brotli = "^1.2.0"
scikit-learn = "~=1.6.1"
optimum = {extras = ["onnxruntime"], version = "^2.1.0"}
azure-storage-blob = "^12.28.0"
welearn-database = "^1.3.0"
pydantic = "^2.12.5"

[tool.poetry.group.metrics.dependencies]
alembic = "^1.18.3"
locust = "^2.43.2"


[tool.poetry.group.dev.dependencies]
mypy = "^1.19.1"
bandit = "^1.9.3"
isort = "^7.0.0"
black = "^26.1.0"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

[[tool.poetry.source]]
name = "pytorch_cpu"
url = "https://download.pytorch.org/whl/cpu"
priority = "explicit"

[tool.poetry.requires-plugins]
poetry-plugin-export = ">=1.8"

[tool.coverage.run]
omit = [
    # omit pytorch-generated files in /tmp
    "/tmp/*",
]
